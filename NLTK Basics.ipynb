{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text into words/tokens (using word_tokenize) or sentence (using sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi my name is Syed.', 'This is a sample text']\n",
      "['Hi', 'my', 'name', 'is', 'Syed', '.', 'This', 'is', 'a', 'sample', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hi my name is Syed. This is a sample text\"\n",
    "sentence_list = sent_tokenize(text)\n",
    "word_list = word_tokenize(text)\n",
    "print(sentence_list)\n",
    "print(word_list) # NOTE: Even punctuations like '.' are also treated as seperate token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'name', 'Syed', 'This', 'sample', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # Stopwords are common words (like 'a', 'an', 'the') which has unnecessary information\n",
    "from string import punctuation # For getting punctuation symbols\n",
    "\n",
    "filter_words = set(stopwords.words('english')+list(punctuation))\n",
    "nonstopword_list = [word for word in word_list if word not in filter_words]\n",
    "print(nonstopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-grams are 2 words/tokens that occour together to form a different meaning. Example: 'Tamil Nadu'\n",
    "\n",
    "# We can identify such words, if we analyse how commonly the words are occuring together. Example: 'Tamil Nadu is a really good place. You should visit Tamil Nadu!'. In this, the word 'Tamil Nadu' has more frequency than the other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Hi', 'name'), 1),\n",
       " (('Syed', 'This'), 1),\n",
       " (('This', 'sample'), 1),\n",
       " (('name', 'Syed'), 1),\n",
       " (('sample', 'text'), 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder # We can also use TrigramCollocationFinder to find Tri-grams\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(nonstopword_list)\n",
    "sorted(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech tagging : To find a verb, noun, adj, ... on in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'NNP'),\n",
       " ('name', 'NN'),\n",
       " ('Syed', 'NNP'),\n",
       " ('This', 'DT'),\n",
       " ('sample', 'NN'),\n",
       " ('text', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tag(nonstopword_list) # Use http://www.nltk.org/book/ch05.html to find the abbrevation of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word sense disambiguation : Idenitfying the meaning of the word based on the context. Example: In sentences 'It was a cool movie' and 'I like tall grass of cool water', the word 'cool' has different meaning depending on the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.01') the lowest part of the musical range\n",
      "Synset('bass.n.02') the lowest part in polyphonic music\n",
      "Synset('bass.n.03') an adult male singer with the lowest voice\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
      "Synset('bass.n.06') the lowest adult male singing voice\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
      "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n",
      "--------------------------------------------------\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn # Wordnet is kind of like thesaurus which has information about word and their relationship\n",
    "from nltk.wsd import lesk # lesk allows to perform word sense disambiguation\n",
    "\n",
    "for synset in wn.synsets('bass'): # synsets allows to get all possible defintion for a given word\n",
    "    print(synset, synset.definition())\n",
    "print(\"-\"*50)\n",
    "word_classified = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"),'bass') # Lesk will identify the correct meaning of the word for a given sentence\n",
    "print(word_classified, word_classified.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stemming : Sometimes just tokenising the words wont be enough. For example: 'closing, closed, closer' will be treated as different words though they have the same meaning as the of these parent word is 'close'. To consider the words are same, we can remove the end words and consider these words as 'clos' (because it is the common word for both 'closing', 'closed' and 'closer').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', 'clos', 'on', 'clos', 'night', 'when', 'she', 'was', 'in', 'the', 'mood', 'to', 'clos', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer # There are lot of stemming alogirthm. LancasterStemmer is one such algorithm\n",
    "\n",
    "inital_text = \"Syed closed the window on closing night when he was in the mood to close.\"\n",
    "stemmer = LancasterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokenize(inital_text)]\n",
    "print(stemmed_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
